{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for a minimum string cover\n",
    "\n",
    "What is the shortest sequence of English words that contains each of the following as a substring? (These are the cities where [Futurice](https://futurice.com/) offices are located.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be on lower case\n",
    "searchkeys = [\"hel\", \"tre\", \"ber\", \"mun\", \"lon\", \"sto\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this more interesting, the search keys can span over word boundaries. That is, the sequence \"willful onion\" covers the search key *lon* because the first word ends with *l* and the second word starts with *on*. The search keys are allowed to overlap, that is, the word \"echelon\" covers both *hel* and *lon* with the letter *l* shared between them.\n",
    "\n",
    "This is an instance of [the set cover problem](https://en.wikipedia.org/wiki/Set_cover_problem) in computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a list of English words from `/usr/share/dict/words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100411 words in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "function load_vocabulary(filename)\n",
    "    map(lowercase,\n",
    "        map(x -> replace(x, \"'\", \"\"),\n",
    "        map(x -> replace(x, \"-\", \"\"),\n",
    "        map(strip, readlines(filename)))))\n",
    "end\n",
    "\n",
    "vocabulary = load_vocabulary(\"/usr/share/dict/words\")\n",
    "\n",
    "println(\"$(length(vocabulary)) words in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simplest thing that could possibly work\n",
    "\n",
    "We could simply form every possible sequence of vocabulary words up to length of six words (at most six words are needed to cover the six search keys), check which of them contains all of the search keys, and keep track of the shortest one. However, that is way too slow because the number of candidates is exponential in the number of search keys. More precisely, the number of candidates equals $length(vocabulary)$ to the power of $length(searchkeys)$.\n",
    "\n",
    "My laptop can evaluate about 10 million candidates in a second. Processing all candidates at that rate would take over 20 000 years. That is slightly longer than I'm willing to wait, so let's see if we can find a faster way.\n",
    "\n",
    "## Greedy approximation\n",
    "\n",
    "Even if we can't (yet) compute the optimal solution efficiently, we can approximate it easily. A greedy approximation groups the vocabulary words according to which search keys they cover, selects the shortest word for each search key, and concatenates the words. This is an approximation because it doesn't find (mostl likely shorter) cases where the search keys strecth over word boundaries. Nevertheless, the greedy approximation is reasonable good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy search: hell trey berg munch lon stop, length: 24\n"
     ]
    }
   ],
   "source": [
    "function cover_greedy(searchkeys, vocabulary)\n",
    "    res = Vector{AbstractString}()\n",
    "    for key in searchkeys\n",
    "        shortest_for_key = sort(filter(w -> contains(w, key), vocabulary), by=length)[1]\n",
    "        push!(res, shortest_for_key)\n",
    "    end\n",
    "\n",
    "    unique(res)\n",
    "end\n",
    "\n",
    "function pretty_search_result(terms)\n",
    "    cost = sum(map(length, terms))    \n",
    "    \"$(join(terms, \" \")), length: $cost\"\n",
    "end\n",
    "\n",
    "greedy_res = cover_greedy(searchkeys, vocabulary)\n",
    "print(\"Greedy search: \")\n",
    "println(pretty_search_result(greedy_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data structures for efficient search\n",
    "\n",
    "Now that we have an approximation, let's focus again on finding the optimal answer.\n",
    "\n",
    "One way to reduce the required computation is to realise that not all word in the vocabulary can appear in a minimal cover. For example, the word \"clock\" can be pruned because it doesn't cover any of the search keys even partially. We can filter out these kind of words. However, care must be taken to keep words that start or end with a partial search key match, such as \"onion\", which starts with a partial match of *lon*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52897 words left after filtering\n"
     ]
    }
   ],
   "source": [
    "flat(A) = mapreduce(x -> isa(x,Array)? flat(x): x, vcat, [], A)\n",
    "\n",
    "function filter_vocabulary(searchkeys, vocabulary)\n",
    "    prefixes = flat(map(s -> [s[1:i] for i in 1:(endof(s)-1)], searchkeys))\n",
    "    postfixes = flat(map(s -> [s[i:endof(s)] for i in 2:endof(s)], searchkeys))\n",
    "    \n",
    "    function covers_key(s::AbstractString)\n",
    "        full = any(x -> contains(s, x), searchkeys)\n",
    "        ends = any(x -> endswith(s, x), prefixes)\n",
    "        starts = any(x -> startswith(s, x), postfixes)\n",
    "        full || ends || starts\n",
    "    end\n",
    "\n",
    "    unique(filter(covers_key, vocabulary))\n",
    "end\n",
    "\n",
    "filtered_vocabulary = filter_vocabulary(searchkeys, vocabulary)\n",
    "println(\"$(length(filtered_vocabulary)) words left after filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another insight is that we only have to cover each key once similar to the greedy approximation. If at any point during the search we know that we have already covered the search key *hel* we can skip all words in the vocabulary that cover *hel* (but only if they don't also cover some other search key!).\n",
    "\n",
    "The function *build_continuations* below builds a dictionary of vocabulry words grouped by the search keys they cover.\n",
    "\n",
    "Search keys crossing word boundaries need special attention. Algorithm below benefits on a fast way to access words that start with a partial search key. So, words starting with *l*, say, are collected in a vector because they can cover *hel* when concatenated with a word that ends with *he*. The algorithm makes an assumption that a search key is covered by at most two consecutive words. The assumption holds for the search keys in this notebook, but is not generally true! A three letter search key with the article *a* in the middle position could be covered by three words. For example, a search key *man* is be covered by the sequence \"warm a nun\". Longer search keys are more likely to violate the assumption. The algorithm still finds a short sequence, but it is no longer guaranteed to be a minimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words covering \"hel\": 244\n",
      "  h-boundary: 339\n",
      "  he-boundary: 2958\n",
      "Words covering \"tre\": 387\n",
      "  t-boundary: 2593\n",
      "  tr-boundary: 3377\n",
      "Words covering \"ber\": 654\n",
      "  b-boundary: 153\n",
      "  be-boundary: 4631\n",
      "Words covering \"mun\": 107\n",
      "  m-boundary: 1375\n",
      "  mu-boundary: 1770\n",
      "Words covering \"lon\": 242\n",
      "  l-boundary: 64\n",
      "  lo-boundary: 1770\n",
      "Words covering \"sto\": 550\n",
      "  s-boundary: 558\n",
      "  st-boundary: 2022\n"
     ]
    }
   ],
   "source": [
    "function build_continuations(\n",
    "        searchkeys::Vector{ASCIIString},\n",
    "        vocabulary::Vector{AbstractString})\n",
    "    by_key = Dict(map(k -> k => Dict{AbstractString, Vector{AbstractString}}(), searchkeys))\n",
    "    for key in searchkeys\n",
    "        by_key[key][\"\"] = sort(filter(w -> contains(w, key), vocabulary), by=length)\n",
    "        \n",
    "        boundaries = map(i -> (key[1:i], key[(i+1):end]), 1:length(key)-1)\n",
    "        for boundary in boundaries\n",
    "            by_key[key][boundary[1]] = sort(filter(w -> startswith(w, boundary[2]), vocabulary), by=length)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    by_key\n",
    "end\n",
    "\n",
    "continuations = build_continuations(searchkeys, filtered_vocabulary)\n",
    "\n",
    "for key in searchkeys\n",
    "    println(\"Words covering \\\"$(key)\\\": $(length(continuations[key][\"\"]))\")\n",
    "    partial = filter(x -> x[1] != \"\", [(boundary, length(terms)) for (boundary, terms) in continuations[key]])\n",
    "    counts = sort(partial, by=x -> length(x[1]))\n",
    "    for (boundary, len) in counts\n",
    "        println(\"  $boundary-boundary: $(len)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth first search\n",
    "\n",
    "The search algorithm must evaluate all sequences of the vocabulary words. The sequences can be thought to form a tree. The inner branches of the tree are partial sequences, such as \"hello nest\" (covers *hel* and *lon*). Its child nodes are sequences that have one word concatenated to the end of the string, such as \"hello nest otter\" (covers *hel*, *lon*, and *sto*) and \"hello nest osbert\" (covers *hel*, *lon*, *sto*, *ber*) among others. The leaf nodes are sequences that cover all the search keys, such as \"hello nest osbert remunerate\".\n",
    "\n",
    "[Depth first search](https://en.wikipedia.org/wiki/Depth-first_search) traverses a search tree as far as possible and then backtracks to explore another branches. While visiting the nodes, the algorithm can be made to keeps track of the shortest feasible sequence encountered so far. The algorithm will find the minimum because it will visit every node eventually.\n",
    "\n",
    "Below is a straight-forward recursive implementation of the depth first search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow depth-first search (with three search keys): munch ell on, length: 10\n"
     ]
    }
   ],
   "source": [
    "function depth_first_search_slow(continuations, searchkeys, prev_word)\n",
    "    best_terms = []\n",
    "    best_cost = 999999\n",
    "\n",
    "    if isempty(searchkeys)\n",
    "        return best_terms, 0\n",
    "    else\n",
    "        for key in searchkeys\n",
    "            if haskey(continuations, key)\n",
    "                for (prev_postfix, candidates) in continuations[key]\n",
    "                    if endswith(prev_word, prev_postfix)\n",
    "                        for next in candidates\n",
    "                            next_cost = length(next)\n",
    "                            filtered_keys = filter(k -> !contains(next, k) && k != key, searchkeys)\n",
    "                            rest_terms, rest_cost = depth_first_search_slow(continuations, filtered_keys, next)\n",
    "                            if next_cost + rest_cost < best_cost\n",
    "                                best_cost = next_cost + rest_cost\n",
    "                                best_terms = vcat([next], rest_terms)\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "            \n",
    "        return best_terms, best_cost\n",
    "    end\n",
    "end\n",
    "\n",
    "function cover_dfs(searchkeys, vocabulary)\n",
    "    continuations = build_continuations(searchkeys, vocabulary)\n",
    "    depth_first_search_slow(continuations, searchkeys, \"\")[1]\n",
    "end\n",
    "\n",
    "dfs_res = cover_dfs([\"hel\", \"lon\", \"mun\"], filtered_vocabulary)\n",
    "print(\"Slow depth-first search (with three search keys): \")\n",
    "println(pretty_search_result(dfs_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this is still too slow to be usable for more than three or four search keys. A rough estimate shows that the search with all six search keys would takes years. (But not thousands of years. We are making progress!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further optimizations\n",
    "\n",
    "### Pruning unnecessary branches\n",
    "\n",
    "Depth first search keeps track of the best solution it has seen so far during the search. We can use that as an upper bound and prune whole branches off of the search tree when it is evident that they can not produce a shorter sequence than the current best solution. Concretely, if the best solution at some point of the search is 24 letters long and the current partial candidate has 20 letters, we can skip all words longer than 3 letters. The *build_continuations_with_keybits* function sorts the candidate lists by word length so that it is possible to skip the remaining of the list as soon as the candidate length grows above the limit.\n",
    "\n",
    "We'll use the greedy solution from the above as in initial upper bound. That way we can already in the beginning of the search prune large parts of the tree.\n",
    "\n",
    "This is not an approximation. Only branches that are guaranteed to not contain the optimal solution are skipped. This will still find the true optimum.\n",
    "\n",
    "### Avoiding memory allocations\n",
    "\n",
    "Profiling the *depth_first_search_slow* function revealed that it spends a considerable proportion of time allocating and releaseing memory. Below is an updated version of the algorithm where I have made several changes to avoid unnecessary memory allocations:\n",
    "* A vector for holding the current evaluation candidate (a vector of words) is allocated once in the beginning and passed as reference to the search function (the *workspace* variable below).\n",
    "* Similarly, the data structure for holding the best solution so far is allocated once (previously the best result was returend by each recursive function call requiring a vector construction on every function call).\n",
    "* The data structure indicating which search keys are unprocessed is replaced by a bit vector (the variable *keybits*). A UInt32 bit vector takes less space than a Vector of Strings that was used before. Manipulating the bit vector is also much faster, particularly, if we precompute keys contained in each word as similar bit vectors. This is done in the *build_continuations_with_keybits* function below. The downside is that bit vectors are limited to at most 32 (or 64, if UInt64s are used) search keys.\n",
    "\n",
    "These changes improve the performance considerably. I profiled each of these changes separately and a few other ideas that turned out not to improve the performance (the intermediate step are not shown here). However, each change also make the code more complicated. The tradeoff between performance and readibilty is often difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a UInt32 bitset indicating which searchkeys are covered by word.\n",
    "# For example, returns 101 if the first and third search key are covered\n",
    "# by the word.\n",
    "function word_to_keybits(word::AbstractString, searchkeys::Vector{ASCIIString})\n",
    "    bits::UInt32 = 0\n",
    "    b::UInt32 = 1\n",
    "    \n",
    "    if length(searchkeys) > 32\n",
    "        error(\"Supports at most 32 search keys\")\n",
    "    end\n",
    "\n",
    "    for key in searchkeys\n",
    "        if contains(word, key)\n",
    "            bits = bits | b\n",
    "        end\n",
    "        b = b << 1\n",
    "    end\n",
    "    \n",
    "    bits\n",
    "end\n",
    "\n",
    "# This is like the build_continuations function above, but also include\n",
    "# bitsets which indicate which search keys are covered by each word.\n",
    "function build_continuations_with_keybits(\n",
    "        searchkeys::Vector{ASCIIString},\n",
    "        vocabulary::Vector{AbstractString})\n",
    "    by_key = Dict(map(k -> k => Dict{AbstractString, Vector{Tuple{AbstractString, UInt32}}}(), searchkeys))\n",
    "    \n",
    "    for key in searchkeys\n",
    "        words = sort(filter(w -> contains(w, key), vocabulary), by=length)\n",
    "        by_key[key][\"\"] = map(w -> (w, word_to_keybits(w, searchkeys)), words)\n",
    "\n",
    "        boundaries = map(i -> (key[1:i], key[(i+1):end]), 1:length(key)-1)\n",
    "        for boundary in boundaries\n",
    "            words = sort(filter(w -> startswith(w, boundary[2]), vocabulary), by=length)\n",
    "            by_key[key][boundary[1]] = map(w -> (w, word_to_keybits(boundary[1] * w, searchkeys)), words)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    by_key\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A struct for representing the current best result\n",
    "type Cover{T}\n",
    "    terms::Vector{T}\n",
    "    cost::Int\n",
    "    level::Int\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "function depth_first_search_optimized!(\n",
    "        searchkeys::Vector{ASCIIString},\n",
    "        keybits::UInt32,\n",
    "        continuations::Dict{ASCIIString, Dict{AbstractString, Vector{Tuple{AbstractString, UInt32}}}},\n",
    "        level::Int,\n",
    "        current_cost::Int,\n",
    "        workspace::Vector{AbstractString},\n",
    "        best::Cover{AbstractString})\n",
    "    if keybits == 0\n",
    "        # The current best solution is updated through a reference to\n",
    "        # avoid the need to copy the vector on every function call.\n",
    "        if current_cost < best.cost\n",
    "            best.cost = current_cost\n",
    "            best.terms[:] = workspace[:]\n",
    "            best.level = level - 1\n",
    "        end\n",
    "    else\n",
    "        if level > 1\n",
    "            prev_word = workspace[level-1]\n",
    "        else\n",
    "            prev_word = \"\"\n",
    "        end\n",
    "\n",
    "        for i in 1:length(searchkeys)\n",
    "            if keybits & (UInt32(1) << (i-1)) == 0\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            key = searchkeys[i]\n",
    "            for (prev_postfix, candidates) in continuations[key]\n",
    "                if endswith(prev_word, prev_postfix)\n",
    "                    for (next, coverbits) in candidates\n",
    "                        next_cost::Int = current_cost + length(next)\n",
    "                        if next_cost > best.cost\n",
    "                            # Every candidate beyond this is too long, \n",
    "                            # because the candidates are sorted by length\n",
    "                            break\n",
    "                        end\n",
    "                        \n",
    "                        # The active keys are updated with a bit operation\n",
    "                        next_keybits = keybits & ~coverbits\n",
    "                        workspace[level] = next\n",
    "                        depth_first_search_optimized!(searchkeys, next_keybits, continuations, level + 1, next_cost, workspace, best)\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Nothing to return here.\n",
    "    # The actual return value is stored into the variable best.\n",
    "    nothing\n",
    "end\n",
    "\n",
    "function all_keys_selected(searchkeys)\n",
    "    bits = UInt32(0)\n",
    "    for i in 1:length(searchkeys)\n",
    "        bits = bits | (UInt32(1) << (i-1))\n",
    "    end\n",
    "    bits\n",
    "end\n",
    "\n",
    "function cover_dfs_optimized(searchkeys, vocabulary)\n",
    "    greedy = cover_greedy(searchkeys, vocabulary)\n",
    "    greedy_cost = sum(map(length, greedy))\n",
    "    best = Cover{AbstractString}(greedy, greedy_cost, length(searchkeys))\n",
    "    workspace = Vector{AbstractString}(length(searchkeys))\n",
    "    keybits = all_keys_selected(searchkeys)\n",
    "    continuations = build_continuations_with_keybits(searchkeys, vocabulary)\n",
    "    depth_first_search_optimized!(searchkeys, keybits, continuations, 1, 0, workspace, best)\n",
    "    best.terms[1:best.level]\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized depth first search: hell on berm unit res to, length: 19\n"
     ]
    }
   ],
   "source": [
    "dfs_res_2 = cover_dfs_optimized(searchkeys, filtered_vocabulary)\n",
    "print(\"Optimized depth first search: \")\n",
    "println(pretty_search_result(dfs_res_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took less than 5 minutes. Quite an improvement compared to the 20 000 years of the naive approach!\n",
    "\n",
    "Notice how efficiently the search keys are being covered. There is only one letter in the solution, *i* in the word *unit*, that is not part of the search keys.\n",
    "\n",
    "There might be other solutions with 19 letters. The code could be modified to visit all solutions that are the same length as the current best solution and store all of them in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you speak Finnish?\n",
    "\n",
    "Now that we have the machinery ready, we can easily repeat the analysis with a different vocabulary. Let's find the shortest sequence of Finnish words containing the same substrings. I downloaded the list of Finnish words from [Kotus](http://kaino.kotus.fi/sanat/nykysuomi/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33531 Finnish words left after filtering\n",
      "Optimized depth first search (Finnish words): helo nyt remu nosto weber, length: 21\n"
     ]
    }
   ],
   "source": [
    "vocabulary_fi = filter_vocabulary(searchkeys, load_vocabulary(\"kotus_sanat.txt\"))\n",
    "println(\"$(length(vocabulary_fi)) Finnish words left after filtering\")\n",
    "\n",
    "dfs_fi = cover_dfs_optimized(searchkeys, vocabulary_fi)\n",
    "print(\"Optimized depth first search (Finnish words): \")\n",
    "println(pretty_search_result(dfs_fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packing is slightly less efficent in Finnish as we have four letters that are not part of the search keys. On the other hand, now we got overlapping search keys *hel* and *lon*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further (untested) optimization ides\n",
    "\n",
    "* Recursive solution constantly re-allocates the stack. The function could be converted into a non-recursive one and the constant-sized space for stack could be allocated once. The upper limit for the required stack space is known because the recursion depth is at most the number of search keys.\n",
    "* Maybe the code could be parallelized? The most important shared state is the best solution found so far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.7",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
